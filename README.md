# ğŸ–¼ï¸ CNN Multi-Class Image Classification on CIFAR-10 | PPGEEC2318

This repository contains the implementation of the **Final Project â€“ Part 1** for the course **PPGEEC2318 â€“ Machine Learning**, instructed by Professor **Ivanovitch Medeiros**. We build, train and evaluate convolutional neural networks (CNNs) on the CIFARâ€‘10 dataset, including data preparation, model definition, training/validation, intermediate feature inspections and final test evaluation.

---

## ğŸ“Œ Project Overview

The goal of this project is to design several CNN architectures for **multiâ€‘class image classification** on the CIFARâ€‘10 dataset. We follow these steps:

1. **Data Preparation & EDA**

   - Load CIFARâ€‘10, visualize sample images
   - Class distribution histograms
   - Standard deviation heatmaps per class

       <!-- Replace with your EDA images -->

     ![Class Distribution Histogram](images/class_distribution_cifar10.png)
     ![Mean Images By Class](images/mean_images_by_class.png)
     ![Std Heatmap per Class](images/std_heatmap_by_class.png)

2. **Model Definition (Base V1)**

   - Three convolutional blocks (Convâ†’BNâ†’ReLUâ†’Convâ†’BNâ†’ReLUâ†’Poolâ†’Dropout)
   - Fullyâ€‘connected head (Flattenâ†’Denseâ†’BNâ†’ReLUâ†’Dropoutâ†’Dense)
   - Implementation in `./cnn_model.ipynb` as `CNNV1`

3. **Training & Validation**

   - `Architecture` class manages train/val loops, loss/metric logging
   - Early stopping, learningâ€‘rate scheduling, augmentation

4. **Filter & Featureâ€‘Map Inspection**

   - Visualize firstâ€‘layer filters  
     ![Conv1 Filters](images/filter_first_conv2d.png)
   - Attach hooks to layers `conv1`, `relu1`, `pool1`, `flatten`
   - Display feature maps for a batch  
     ![Feature Maps](images/feature_maps.png)

5. **Final Test Evaluation**
   - Evaluate on unseen CIFARâ€‘10 test set
   - Compute test accuracy
   - Plot confusion matrix  
     ![Confusion Matrix](images/confusion_matrix_test_set.png)

---

## ğŸ› ï¸ Technologies Used

- **Python 3.8+**
- **PyTorch** & **torchvision**
- **Scikitâ€‘learn** (metrics, confusion matrix)
- **Matplotlib** & **Seaborn** (visualizations)

---

## ğŸ“ Repository Structure

```

.
â”œâ”€â”€ data/               # CIFARâ€‘10 download scripts or pointers
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ eda\_class\_dist.png
â”‚   â”œâ”€â”€ eda\_std\_heatmap.png
â”‚   â”œâ”€â”€ cnn\_filters.png
â”‚   â”œâ”€â”€ feature\_maps.png
â”‚   â””â”€â”€ confusion\_matrix.png
â”œâ”€â”€ eda.ipynb
â”œâ”€â”€ Architecture.py
â”œâ”€â”€ cnn_model.ipynb          # contains CNNV1 class
â”œâ”€â”€ requirements.txt        # project dependencies
â””â”€â”€ README.md               # this file

```

---

# ğŸ“ Model Card â€“ CIFARâ€‘10 CNN Classifier

Model cards provide context, usage guidelines, and limitations of ML models.

---

## ğŸ“Œ Model Details

- **Author:** Efrain Marcelo
- **Framework:** PyTorch
- **Architecture:** `CNNV1` â€“ three convolutional blocks with batchâ€‘norm, ReLU, pooling and dropout; twoâ€‘layer MLP head
- **Training pipeline:** managed by custom `Architecture` class for reproducible train/val loops, early stopping, LR scheduling, hooks

---

## ğŸ¯ Intended Use

This model is intended for **educational** and **experimental** purposes, demonstrating:

- Endâ€‘toâ€‘end CNN pipeline on CIFARâ€‘10
- Data augmentation strategies
- Inspection of learned filters and intermediate activations
- Handling of train/validation/test splits

**Not** for realâ€‘world image classification deployment without further tuning and robustness checks.

---

## ğŸ§ª Training Data

- **Dataset:** CIFARâ€‘10 (60â€¯000 color images, 10 classes)
- **Split:** 80% train, 20% validation (random split), separate 10â€¯000 test images unseen during training
- **Augmentation:** RandomCrop(32, pad=4), RandomHorizontalFlip

---

## ğŸ§ª Evaluation Data

- **Validation:** used for early stopping and LR scheduling
- **Test:** final unseen set for metrics

---

## ğŸ“ˆ Metrics

- **Test Accuracy:** 0.7422
- **Confusion Matrix:** see above for classâ€‘wise performance

---

## âš–ï¸ Ethical Considerations

- CIFARâ€‘10 classes (e.g., â€œcatâ€, â€œdogâ€, â€œtruckâ€) are generic and carry minimal sensitive bias.
- Nevertheless, the pipeline illustrates general methods that could be misapplied to sensitive data in production.

---

## âš ï¸ Caveats and Recommendations

- **Overfitting risk:** monitor train vs. val loss gap; use early stopping and augmentations.
- **Further improvements:** deeper architectures, transfer learning (e.g., ResNetâ€‘18), advanced augmentations (CutMix, MixUp), extensive hyperparameter search.
- **Interpretability:** use hooks, filter visualizations, and SHAP/LIME for deeper analysis.
